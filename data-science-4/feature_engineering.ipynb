{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature engineering\n",
    "\n",
    "Parte importante do fluxo de trabalho de pré-processamento.\n",
    "Feature engineering é a criação de novos recursos com base nas features existentes.\n",
    "\n",
    "\"Feature engineering is the process of transforming raw data into features that better represent the underlying problem to the predictive models, resulting in improved model accuracy on unseen data.\"\n",
    "\n",
    "Machine learning algorithms learn a solution to a problem from sample data.\n",
    "\n",
    "In this context, feature engineering asks: what is the best representation of the sample data to learn a solution to your problem?\n",
    "\n",
    "A feature is an attribute that is useful or meaningful to your problem. It is an important part of an observation for learning about the structure of the problem that is being modeled.\n",
    "\n",
    "Feature importance scores can also provide you with information that you can use to extract or construct new features, similar but different to those that have been estimated to be useful.\n",
    "\n",
    "A feature may be important if it is highly correlated with the dependent variable (the thing being predicted). Correlation coefficients and other univariate (each attribute is considered independently) methods are common methods.\n",
    "\n",
    "## List of Techniques\n",
    "\n",
    "* imputation\n",
    "\n",
    "* handling outliers\n",
    "\n",
    "* binning\n",
    "\n",
    "* log transformation\n",
    "\n",
    "* one-hot encoding\n",
    "\n",
    "* grouping operations\n",
    "\n",
    "* feature split\n",
    "\n",
    "* scaling\n",
    "\n",
    "* extracting date\n",
    "\n",
    "To see the features used in the analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(path_to_csv_file)\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### data types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df.dtypes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Selecting specific data types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.select_dtypes(include = ['int', 'float'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dealing with Categorical Variables\n",
    "\n",
    "To use categorical variables in a machine learning model, you first need to represent them in a quantitative way. The two most common approaches are to one-hot encode the variables using or to use dummy variables.\n",
    "\n",
    "#### Enconding categorical features\n",
    "\n",
    "* One-hot enconding\n",
    "* Dummy enconding\n",
    "\n",
    "by default in Pandas - one-hot, using the function get_dummies().\n",
    "\n",
    "A codificação one-hot converte n categorias em n recursos.\n",
    "A função pega um dataframe e uma lista de colunas categóricas e retorna um dataframe atualizado com essas colunas incluídas. A especificação de um prefixo com o argumento prefix pode melhorar a legibilidade.\n",
    "\n",
    "A codificação **Dummy enconding** cria recursos n - 1, para n categorias, omitindo a primeira categoria.\n",
    "\n",
    "#### One-hot vs. dummies\n",
    "\n",
    "* One-hot enconding: explainable features - can create a colinear features as result of the same information appear multiple times.\n",
    "\n",
    "* Dummy-enconding: Necessary information without duplication\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Para saber o número de categorias de uma coluna específica usar:\n",
    "counts = df['categorical feature'].value_counts()\n",
    "print(counts)\n",
    "\n",
    "# Para limitar o número de códigos podemos criar uma máscara\n",
    "# Uma máscara é uma lista de booleanos que descrevem quais \n",
    "# valores em uma coluna devem ser afetados\n",
    "\n",
    "mask = df['categorical feature'].isin(counts[counts < 5].index)\n",
    "df['categorical feature'][mask] = 'Other'\n",
    "print(pd.value_counts(xxx))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.get_dummies(df, columns = ['Categoric feature'], prefix = 'C')\n",
    "\n",
    "pd.get_dummies(df, columns = ['list of catecorical features'],\n",
    "                              drop_first = True, prefix = 'M')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>key</th>\n",
       "      <th>data1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>b</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>b</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>a</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>c</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>a</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>b</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  key  data1\n",
       "0   b      0\n",
       "1   b      1\n",
       "2   a      2\n",
       "3   c      3\n",
       "4   a      4\n",
       "5   b      5"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.DataFrame({'key' : ['b', 'b', 'a', 'c', 'a', 'b'],\n",
    "                  'data1': range(6)})\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>key_a</th>\n",
       "      <th>key_b</th>\n",
       "      <th>key_c</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   key_a  key_b  key_c\n",
       "0      0      1      0\n",
       "1      0      1      0\n",
       "2      1      0      0\n",
       "3      0      0      1\n",
       "4      1      0      0\n",
       "5      0      1      0"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dummies = pd.get_dummies(df['key'], prefix = 'key')\n",
    "dummies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>data1</th>\n",
       "      <th>key_a</th>\n",
       "      <th>key_b</th>\n",
       "      <th>key_c</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   data1  key_a  key_b  key_c\n",
       "0      0      0      1      0\n",
       "1      1      0      1      0\n",
       "2      2      1      0      0\n",
       "3      3      0      0      1\n",
       "4      4      1      0      0\n",
       "5      5      0      1      0"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[['data1']].join(dummies)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Enconding binary variables - Pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "users['sub_enc'] = users['subscribed'].apply(lambda val: 1 if val 'y' else 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Enconding binary variables - scikit - learn\n",
    "**LabelEncoder**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# Set up the LabelEncoder object\n",
    "enc = LabelEncoder()\n",
    "\n",
    "# Apply the encoding to the \"Accessible\" column\n",
    "hiking[\"Accessible_enc\"] = enc.fit_transform(hiking[\"Accessible\"])\n",
    "\n",
    "# Compare the two columns\n",
    "print(hiking[[\"Accessible_enc\", \"Accessible\"]].head())\n",
    "#.fit_transform() is a good way to both fit an encoding and transform the data in a single step."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Numerical variables\n",
    "\n",
    "While numeric values can often be used without any feature engineering, there will be cases when some form of manipulation can be useful. For example on some occasions, you might not care about the magnitude of a value but only care about its direction, or if it exists at all. In these situations, you will want to binarize a column. \n",
    "\n",
    "\n",
    "### Binning values\n",
    "\n",
    "For many continuous values you will care less about the exact value of a numeric column, but instead care about the bucket it falls into. This can be useful when plotting values, or simplifying your machine learning models. It is mostly used on continuous variables where accuracy is not the biggest concern e.g. age, height, wages.\n",
    "\n",
    "Bins are created using pd.cut(df['column_name'], bins) where bins can be an integer specifying the number of evenly spaced bins, or a list of bin boundaries."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dados contínuos com frequência são discretizados ou , de modo alternativo, separados em 'compartimentos' (bins) para análise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(18, 25], (18, 25], (18, 25], (25, 35], (18, 25], ..., (25, 35], (60, 100], (35, 60], (35, 60], (25, 35]]\n",
       "Length: 12\n",
       "Categories (4, interval[int64]): [(18, 25] < (25, 35] < (35, 60] < (60, 100]]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ages = [20, 22 , 25, 27, 21, 23, 37, 31, 61, 45, 41, 32]\n",
    "\n",
    "bins = [18, 25, 35, 60, 100]\n",
    "\n",
    "categorias = pd.cut(ages, bins)\n",
    "categorias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, 1, 0, 0, 2, 1, 3, 2, 2, 1], dtype=int8)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "categorias.codes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "IntervalIndex([(18, 25], (25, 35], (35, 60], (60, 100]],\n",
       "              closed='right',\n",
       "              dtype='interval[int64]')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "categorias.categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(18, 25]     5\n",
       "(35, 60]     3\n",
       "(25, 35]     3\n",
       "(60, 100]    1\n",
       "dtype: int64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.value_counts(categorias)  # contadores de compartimentos para o resultado de pd.cut"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[juventude, juventude, juventude, jovem adulto, juventude, ..., jovem adulto, senior, idade média, idade média, jovem adulto]\n",
       "Length: 12\n",
       "Categories (4, object): [juventude < jovem adulto < idade média < senior]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "group_names = ['juventude', 'jovem adulto', 'idade média', 'senior']\n",
    "pd.cut(ages, bins, labels = group_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0.026, 0.26], (0.026, 0.26], (0.72, 0.95], (0.49, 0.72], (0.72, 0.95], ..., (0.49, 0.72], (0.49, 0.72], (0.49, 0.72], (0.72, 0.95], (0.49, 0.72]]\n",
       "Length: 20\n",
       "Categories (4, interval[float64]): [(0.026, 0.26] < (0.26, 0.49] < (0.49, 0.72] < (0.72, 0.95]]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data  = np.random.rand(20)\n",
    "pd.cut(data, 4, precision = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = list(range(1, 101))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0.999, 25.5], (0.999, 25.5], (0.999, 25.5], (0.999, 25.5], (0.999, 25.5], ..., (74.5, 99.0], (74.5, 99.0], (74.5, 99.0], (74.5, 99.0], (74.5, 99.0]]\n",
       "Length: 99\n",
       "Categories (4, interval[float64]): [(0.999, 25.5] < (25.5, 50.0] < (50.0, 74.5] < (74.5, 99.0]]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cats = pd.qcut(data, 4) # separa em quantis\n",
    "cats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(74.5, 99.0]     25\n",
       "(25.5, 50.0]     25\n",
       "(0.999, 25.5]    25\n",
       "(50.0, 74.5]     24\n",
       "dtype: int64"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.value_counts(cats)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Binning  \n",
    "\n",
    "Binning can be applied on both categorical and numerical data:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Numerical Binning Example\n",
    "Value      Bin       \n",
    "0-30   ->  Low       \n",
    "31-70  ->  Mid       \n",
    "71-100 ->  High\n",
    "#Categorical Binning Example\n",
    "Value      Bin       \n",
    "Spain  ->  Europe      \n",
    "Italy  ->  Europe       \n",
    "Chile  ->  South America\n",
    "Brazil ->  South America"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The main motivation of binning is to make the model more robust and prevent overfitting, however, it has a cost to the performance. Every time you bin something, you sacrifice information and make your data more regularized. (Please see regularization in machine learning)\n",
    "\n",
    "The trade-off between performance and overfitting is the key point of the binning process. In my opinion, for numerical columns, except for some obvious overfitting cases, binning might be redundant for some kind of algorithms, due to its effect on model performance.\n",
    "\n",
    "However, for categorical columns, the labels with low frequencies probably affect the robustness of statistical models negatively. Thus, assigning a general category to these less frequent values helps to keep the robustness of the model. For example, if your data size is 100,000 rows, it might be a good option to unite the labels with a count less than 100 to a new category like “Other”."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Numerical Binning Example\n",
    "data['bin'] = pd.cut(data['value'], bins=[0,30,70,100], labels=[\"Low\", \"Mid\", \"High\"])\n",
    "   value   bin\n",
    "0      2   Low\n",
    "1     45   Mid\n",
    "2      7   Low\n",
    "3     85  High\n",
    "4     28   Low\n",
    "#Categorical Binning Example\n",
    "     Country\n",
    "0      Spain\n",
    "1      Chile\n",
    "2  Australia\n",
    "3      Italy\n",
    "4     Brazil\n",
    "conditions = [\n",
    "    data['Country'].str.contains('Spain'),\n",
    "    data['Country'].str.contains('Italy'),\n",
    "    data['Country'].str.contains('Chile'),\n",
    "    data['Country'].str.contains('Brazil')]\n",
    "\n",
    "choices = ['Europe', 'Europe', 'South America', 'South America']\n",
    "\n",
    "data['Continent'] = np.select(conditions, choices, default='Other')\n",
    "     Country      Continent\n",
    "0      Spain         Europe\n",
    "1      Chile  South America\n",
    "2  Australia          Other\n",
    "3      Italy         Europe\n",
    "4     Brazil  South America"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Um método comum de feature engineering é usar um conjunto agregado(média) para usar no lugar dessas features. Isso pode ser útil para reduzir a dimensionalidade do espaço de features\n",
    "\n",
    "Usando uma estatística agregada como a média"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = ['day1', 'day2', 'day3']\n",
    "df['media'] = df.apply(lambda row: row[columns].mean(), axis = 1)\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dealing with missing values 1\n",
    "### Listwise deletion in Python\n",
    "\n",
    "Serve para variáveis categóricas aonde os dados ausentes ocorrem aleatoriamente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop all rows with at least one missing values\n",
    "df.dropna(how = 'any')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop rows with missing values in a specific column\n",
    "df.dropna(subset = ['VersionControl'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "No caso de colunas categóricas, é comum substituir valores ausentes por sequências de caracteres como 'outro', 'não fornecido',ect...."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace missing values in a specific column with a given string\n",
    "df['VersionControl'].fillna(value = 'None Given', inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Record where the values are not misssing\n",
    "df['SalaryGiven'] = df ['ConvertedSalary'].notnull()\n",
    "\n",
    "# Drop a specific column\n",
    "df.drop(columns = ['ConvertedSalary'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Usando o booleano pra deletar linhas específicas aonde está faltando dado\n",
    "print(df[df['B'] == 7])\n",
    "# Contando o número de elementos faltantes numa especifica coluna\n",
    "print(df['col_name'].isnull().sum())\n",
    "\n",
    "# mostrando os elementos não nulos em uma coluna específica\n",
    "print(df['col_name'].notnull())\n",
    "\n",
    "dropna('col_name', axis = 1, thresh = 3)\n",
    "\n",
    "# Check how many values are missing in the category_desc column\n",
    "print(volunteer['category_desc'].isnull().sum())\n",
    "\n",
    "# Subset the volunteer dataset\n",
    "volunteer_subset = volunteer[volunteer['category_desc'].notnull()]\n",
    "\n",
    "# Print out the shape of the subset\n",
    "print(volunteer_subset.shape)\n",
    "\n",
    "# Imputation\n",
    "threshold = 0.7\n",
    "#Dropping columns with missing value rate higher than threshold\n",
    "data = data[data.columns[data.isnull().mean() < threshold]]\n",
    "\n",
    "#Dropping rows with missing value rate higher than threshold\n",
    "data = data.loc[data.isnull().mean(axis=1) < threshold]\n",
    "\n",
    "#Filling all missing values with 0\n",
    "data = data.fillna(0)\n",
    "#Filling missing values with medians of the columns\n",
    "data = data.fillna(data.median())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dealing with missing values 2\n",
    "## Fill continuos missing values\n",
    "### Dealing with missing values(2)\n",
    "\n",
    "* categorical columns: replace missing values with the most common occurring value or with a string that flags missing values such as 'None'\n",
    "\n",
    "* numerical columns: replace missing values with a suitable values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['column X'] = df['Column X'].fillna(df['Column X'].mean())\n",
    "\n",
    "#Max fill function for categorical columns\n",
    "data['column_name'].fillna(data['column_name'].value_counts()\n",
    ".idxmax(), inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dealing with bad characteres"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['RawSalary'] = df['RawSalary'].str.replace(',', '')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the last exercise, you could tell quickly based off of the df.head() call which characters were causing an issue. In many cases this will not be so apparent. There will often be values deep within a column that are preventing you from casting a column as a numeric type so that it can be used in a model or further feature engineering.\n",
    "\n",
    "One approach to finding these values is to force the column to the data type desired using pd.to_numeric(), coercing any values causing issues to NaN, Then filtering the DataFrame by just the rows containing the NaN values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Attempt to convert the column to numeric values\n",
    "numeric_vals = pd.to_numeric(so_survey_df['RawSalary'], errors='coerce')\n",
    "\n",
    "# Find the indexes of missing values\n",
    "idx = numeric_vals.isna()\n",
    "\n",
    "# Print the relevant rows\n",
    "print(so_survey_df['RawSalary'][idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# conversão de tipo \n",
    "df['RawSalary'] = df['RawSalary'].astype('float') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Chaining methods\n",
    "\n",
    "df['column_name'] = df['column_name'].method1().method2().method3()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use method chaining\n",
    "so_survey_df['RawSalary'] = so_survey_df['RawSalary']\\\n",
    "                              .str.replace(',', '')\\\n",
    "                              .str.replace('$', '')\\\n",
    "                              .str.replace('£', '')\\\n",
    "                              .astype('float')\n",
    " \n",
    "# Print the RawSalary column\n",
    "print(so_survey_df['RawSalary'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Uma importante consideração antes de criar um modelo de machine learning é entender como a distribuição dos dados subjacentes. Muitos algoritmos fazem suposições sobre como seus dados são distribuídos ou como as diferentes features interagem entre si. Por exemplo, quase todos os modelos, além dos modelos baseados em árvore, exigem que as features estejam na mesma escala. \n",
    "A feature engineering pode ser usada para manipular os dados para que possam caber nas suposições de distribuição, ou pelo menos, ajustam-na o mais próximo possível. Quase todos os modelos, além dos modelos baseados em árvore, assumem que seus dados sejam normalmente distribuídos.\n",
    "\n",
    "Para entender a forma de seus próprios dados, vc pode criar histogramas de cada um das features contínuas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Para gerar um histograma só das colunas numéricas\n",
    "# separar essas colunas em um novo dataframe e então usar\n",
    "\n",
    "# import matplotlib as plt\n",
    "\n",
    "df.hist()\n",
    "plt.show()\n",
    "\n",
    "# Para gerar um boxplot no pandas\n",
    "# Create a boxplot of two columns\n",
    "so_numeric_df[['Age', 'Years Experience']].boxplot()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Min-Max scaling\n",
    "\n",
    "A escala Min-Max é quando seus dados são dimensionados linearmente entre um valor mínimo e máximo. Como é uma escala linear enquanto os valores mudam, a distribuição não.\n",
    "\n",
    "When could you use normalization(MinMaxScaler) when working with a dataset?\n",
    "Normalization scales all points linearly between the upper and lower bound."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Min-Max scaling in Python\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "# pre processing modulo - sckit learn é a biblioteca de ML mais usada \n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "\n",
    "scaler.fit(df[['column']])\n",
    "\n",
    "df['normalized_column'] = scaler.transform(df[['column']])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Standardization\n",
    "\n",
    "Outro escalonador comumente usado é chamado de padronização. A padronização encontra a média dos seus dados e centraliza a sua distribuição ao redor, calculando o número de desvios-padrão da média em cada ponto."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Standardization in Python\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "scaler = StandardScaler()\n",
    "\n",
    "scaler.fit(df[['col']])\n",
    "\n",
    "df['standardized_col'] = scaler\\\n",
    "                         .transform(df[['col']])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Log Transformation\n",
    "\n",
    "Uma transformação de log, por outro lado, pode ser usada para tornar as distribuições high skewed em less skewed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Logarithm transformation (or log transform) is one of the most commonly used mathematical transformations in feature engineering. What are the benefits of log transform:\n",
    "\n",
    "* It helps to handle skewed data and after transformation, the distribution becomes more approximate to normal.\n",
    "\n",
    "* In most of the cases the magnitude order of the data changes within the range of the data. For instance, the difference between ages 15 and 20 is not equal to the ages 65 and 70. In terms of years, yes, they are identical, but for all other aspects, 5 years of difference in young ages mean a higher magnitude difference. This type of data comes from a multiplicative process and log transform normalizes the magnitude differences like that.\n",
    "\n",
    "* It also decreases the effect of the outliers, due to the normalization of magnitude differences and the model become more robust.\n",
    "\n",
    "**A critical note**: The data you apply log transform must have only positive values, otherwise you receive an error. Also, you can add 1 to your data before transform it. Thus, you ensure the output of the transformation to be positive."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>value</th>\n",
       "      <th>log+1</th>\n",
       "      <th>log</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2</td>\n",
       "      <td>1.098612</td>\n",
       "      <td>3.258097</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>45</td>\n",
       "      <td>3.828641</td>\n",
       "      <td>4.234107</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-23</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>85</td>\n",
       "      <td>4.454347</td>\n",
       "      <td>4.691348</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>28</td>\n",
       "      <td>3.367296</td>\n",
       "      <td>3.951244</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2</td>\n",
       "      <td>1.098612</td>\n",
       "      <td>3.258097</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>35</td>\n",
       "      <td>3.583519</td>\n",
       "      <td>4.077537</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>-12</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.484907</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   value     log+1       log\n",
       "0      2  1.098612  3.258097\n",
       "1     45  3.828641  4.234107\n",
       "2    -23       NaN  0.000000\n",
       "3     85  4.454347  4.691348\n",
       "4     28  3.367296  3.951244\n",
       "5      2  1.098612  3.258097\n",
       "6     35  3.583519  4.077537\n",
       "7    -12       NaN  2.484907"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Log Transform Example\n",
    "data = pd.DataFrame({'value':[2,45, -23, 85, 28, 2, 35, -12]})\n",
    "data['log+1'] = (data['value']+1).transform(np.log)\n",
    "#Negative Values Handling\n",
    "#Note that the values are different\n",
    "data['log'] = (data['value']-data['value'].min()+1) .transform(np.log)\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Log transformation in Python\n",
    "\n",
    "from sklearn.preprocessing import PowerTransformer\n",
    "\n",
    "log = PowerTransformer()\n",
    "\n",
    "log.fit(df['col'])\n",
    "\n",
    "df['log_col'] = log.transform(df[['col']])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Removing outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Quantiles in Python\n",
    "\n",
    "q_cutoff = df['col_name'].quantile(0.95)\n",
    "\n",
    "mask = df['col_name'] < q_cutoff\n",
    "\n",
    "trimmed_df = df[mask]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Standard deviation based detection\n",
    "baseado na distância a partir da média.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Standard deviation detection in Python\n",
    "\n",
    "mean = df['col_name'].mean()\n",
    "std = df['col_name'].std()\n",
    "\n",
    "cut_off = std * 3\n",
    "\n",
    "lower, upper = mean - cut_off, mean + cut_off\n",
    "\n",
    "new_df = df[(df['col_name'] < upper) & (df['col_name'] > lower)]\n",
    "\n",
    "#Dropping the outlier rows with standard deviation\n",
    "factor = 3\n",
    "upper_lim = data['column'].mean () + data['column'].std () * factor\n",
    "lower_lim = data['column'].mean () - data['column'].std () * factor\n",
    "\n",
    "data = data[(data['column'] < upper_lim) & (data['column'] > lower_lim)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### An Outlier Dilemma: Drop or Cap\n",
    "\n",
    "Another option for handling outliers is to cap them instead of dropping. So you can keep your data size and at the end of the day, it might be better for the final model performance.\n",
    "On the other hand, capping can affect the distribution of the data, thus it better not to exaggerate it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Capping the outlier rows with Percentiles\n",
    "upper_lim = data['column'].quantile(.95)\n",
    "lower_lim = data['column'].quantile(.05)\n",
    "data.loc[(df[column] > upper_lim),column] = upper_lim\n",
    "data.loc[(df[column] < lower_lim),column] = lower_lim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Extracting Date\n",
    "\n",
    "Though date columns usually provide valuable information about the model target, they are neglected as an input or used nonsensically for the machine learning algorithms. It might be the reason for this, that dates can be present in numerous formats, which make it hard to understand by algorithms, even they are simplified to a format like \"01–01–2017\".\n",
    "\n",
    "Building an ordinal relationship between the values is very challenging for a machine learning algorithm if you leave the date columns without manipulation. Here, I suggest three types of preprocessing for dates:\n",
    "\n",
    "* Extracting the parts of the date into different columns: Year, month, day, etc.\n",
    "* Extracting the time period between the current date and columns in terms of years, months, days, etc.\n",
    "* Extracting some specific features from the date: Name of the weekday, Weekend or not, holiday or not, etc.\n",
    "\n",
    "If you transform the date column into the extracted columns like above, the information of them become disclosed and machine learning algorithms can easily understand them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>year</th>\n",
       "      <th>month</th>\n",
       "      <th>passed_years</th>\n",
       "      <th>passed_months</th>\n",
       "      <th>day_name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2017-01-01</td>\n",
       "      <td>2017</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>40</td>\n",
       "      <td>Sunday</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2008-12-04</td>\n",
       "      <td>2008</td>\n",
       "      <td>12</td>\n",
       "      <td>12</td>\n",
       "      <td>137</td>\n",
       "      <td>Thursday</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1988-06-23</td>\n",
       "      <td>1988</td>\n",
       "      <td>6</td>\n",
       "      <td>32</td>\n",
       "      <td>383</td>\n",
       "      <td>Thursday</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1999-08-25</td>\n",
       "      <td>1999</td>\n",
       "      <td>8</td>\n",
       "      <td>21</td>\n",
       "      <td>249</td>\n",
       "      <td>Wednesday</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1993-02-20</td>\n",
       "      <td>1993</td>\n",
       "      <td>2</td>\n",
       "      <td>27</td>\n",
       "      <td>327</td>\n",
       "      <td>Saturday</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        date  year  month  passed_years  passed_months   day_name\n",
       "0 2017-01-01  2017      1             3             40     Sunday\n",
       "1 2008-12-04  2008     12            12            137   Thursday\n",
       "2 1988-06-23  1988      6            32            383   Thursday\n",
       "3 1999-08-25  1999      8            21            249  Wednesday\n",
       "4 1993-02-20  1993      2            27            327   Saturday"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datetime import date\n",
    "\n",
    "data = pd.DataFrame({'date':\n",
    "['01-01-2017',\n",
    "'04-12-2008',\n",
    "'23-06-1988',\n",
    "'25-08-1999',\n",
    "'20-02-1993',\n",
    "]})\n",
    "\n",
    "#Transform string to date\n",
    "data['date'] = pd.to_datetime(data.date, format=\"%d-%m-%Y\")\n",
    "\n",
    "#Extracting Year\n",
    "data['year'] = data['date'].dt.year\n",
    "\n",
    "#Extracting Month\n",
    "data['month'] = data['date'].dt.month\n",
    "\n",
    "#Extracting passed years since the date\n",
    "data['passed_years'] = date.today().year - data['date'].dt.year\n",
    "\n",
    "#Extracting passed months since the date\n",
    "data['passed_months'] = (date.today().year - data['date'].dt.year) * 12 + date.today().month - data['date'].dt.month\n",
    "\n",
    "#Extracting the weekday name of the date\n",
    "data['day_name'] = data['date'].dt.day_name()\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Dates\n",
    "\n",
    "df['data_converted'] = pd.to_datetime(df['date'])\n",
    "\n",
    "df['month'] = df[\"date_converted\"].apply(lambda row: row.month)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Stratified Sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a data with all columns except category_desc\n",
    "volunteer_X = volunteer.drop('category_desc', axis= 1)\n",
    "\n",
    "# Create a category_desc labels dataset\n",
    "volunteer_y = volunteer[['category_desc']]\n",
    "\n",
    "# Use stratified sampling to split up the dataset according to the volunteer_y dataset\n",
    "X_train, X_test, y_train, y_test = train_test_split(volunteer_X, volunteer_y, stratify = volunteer_y)\n",
    "\n",
    "# Print out the category_desc counts on the training y labels\n",
    "print(y_train[\"category_desc\"].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Grouping Operations\n",
    "\n",
    "In most machine learning algorithms, every instance is represented by a row in the training dataset, where every column show a different feature of the instance. This kind of data called “Tidy”.\n",
    "\n",
    "\" Tidy datasets are easy to manipulate, model and visualise, and have a specific structure: each variable is a column, each observation is a row, and each type of observational unit is a table.\"\n",
    "\n",
    "Datasets such as transactions rarely fit the definition of tidy data above, because of the multiple rows of an instance. In such a case, we group the data by the instances and then every instance is represented by only one row.\n",
    "\n",
    "The key point of group by operations is to decide the aggregation functions of the features. For numerical features, average and sum functions are usually convenient options, whereas for categorical features it more complicated.\n",
    "\n",
    "##### Categorical Column Grouping\n",
    "\n",
    "* The first option is to select the label with the highest frequency. In other words, this is the max operation for categorical columns, but ordinary max functions generally do not return this value, you need to use a lambda function for this purpose.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.groupby('id').agg(lambda x: x.value_counts().index[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Second option is to make a pivot table. This approach resembles the encoding method in the preceding step with a difference. Instead of binary notation, it can be defined as aggregated functions for the values between grouped and encoded columns. This would be a good option if you aim to go beyond binary flag columns and merge multiple features into aggregated features, which are more informative."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Pivot table Pandas Example\n",
    "data.pivot_table(index='column_to_group', columns='column_to_encode', values='aggregation_column', \n",
    "                 aggfunc=np.sum, fill_value = 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Last categorical grouping option is to apply a group by function after applying one-hot encoding. This method preserves all the data -in the first option you lose some-, and in addition, you transform the encoded column from categorical to numerical in the meantime. You can check the next section for the explanation of numerical column grouping."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Numerical Column Grouping\n",
    "\n",
    "Numerical columns are grouped using sum and mean functions in most of the cases. Both can be preferable according to the meaning of the feature. For example, if you want to obtain ratio columns, you can use the average of binary columns. In the same example, sum function can be used to obtain the total count either."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sum_cols: List of columns to sum\n",
    "#mean_cols: List of columns to average\n",
    "grouped = data.groupby('column_to_group')\n",
    "\n",
    "sums = grouped[sum_cols].sum().add_suffix('_sum')\n",
    "avgs = grouped[mean_cols].mean().add_suffix('_avg')\n",
    "\n",
    "new_df = pd.concat([sums, avgs], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Feature Split\n",
    "\n",
    "Splitting features is a good way to make them useful in terms of machine learning. Most of the time the dataset contains string columns that violates tidy data principles. By extracting the utilizable parts of a column into new features:\n",
    "\n",
    "* We enable machine learning algorithms to comprehend them.\n",
    "* Make possible to bin and group them.\n",
    "* Improve model performance by uncovering potential information.\n",
    "\n",
    "Split function is a good option, however, there is no one way of splitting features. It depends on the characteristics of the column, how to split it. Let’s introduce it with two examples. First, a simple split function for an ordinary name column:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.name\n",
    "0  Luther N. Gonzalez\n",
    "1    Charles M. Young\n",
    "2        Terry Lawson\n",
    "3       Kristen White\n",
    "4      Thomas Logsdon\n",
    "#Extracting first names\n",
    "data.name.str.split(\" \").map(lambda x: x[0])\n",
    "0     Luther\n",
    "1    Charles\n",
    "2      Terry\n",
    "3    Kristen\n",
    "4     Thomas\n",
    "#Extracting last names\n",
    "data.name.str.split(\" \").map(lambda x: x[-1])\n",
    "0    Gonzalez\n",
    "1       Young\n",
    "2      Lawson\n",
    "3       White\n",
    "4     Logsdon"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The example above handles the names longer than two words by taking only the first and last elements and it makes the function robust for corner cases, which should be regarded when manipulating strings like that.\n",
    "\n",
    "Another case for split function is to extract a string part between two chars. The following example shows an implementation of this case by using two split functions in a row."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#String extraction example\n",
    "data.title.head()\n",
    "0                      Toy Story (1995)\n",
    "1                        Jumanji (1995)\n",
    "2               Grumpier Old Men (1995)\n",
    "3              Waiting to Exhale (1995)\n",
    "4    Father of the Bride Part II (1995)\n",
    "data.title.str.split(\"(\", n=1, expand=True)[1].str.split(\")\", n=1, expand=True)[0]\n",
    "0    1995\n",
    "1    1995\n",
    "2    1995\n",
    "3    1995\n",
    "4    1995"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
